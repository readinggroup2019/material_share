{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmqR2pgawfBb"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peHuxA-9d-Gn"
   },
   "source": [
    "# 2. AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "[A useful artical](https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/)\n",
    "\n",
    "Central to all neural networks in PyTorch is the ```autograd``` package, which **provides automatic differentiation for all operations on Tensors**.\n",
    "\n",
    "\n",
    "Mathematically, if you have a vector valued function $\\tilde{y}=f(\\tilde{x})$, then the gradient of with respect to $x$ is a Jacobian matrix\n",
    "$$\n",
    "\\begin{split}J=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}\n",
    "$$\n",
    "Generally speaking, ```torch.autograd``` is an engine for computing vector-Jacobian product. That is, given any vector $v$ compute\n",
    "the product $v^{T}\\cdot J$.\n",
    "\n",
    "If $v$ is the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$:\n",
    "$$v=\\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$$\n",
    "By the chain rule, the vector-Jacobian product would be the gradient of $l$ with respect to $\\tilde{x}$ :\n",
    "$$\\begin{split}J^{T}\\cdot v=\\left(\\begin{array}{ccc}\n",
    " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    " \\vdots & \\ddots & \\vdots\\\\\n",
    " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial y_{m}}\n",
    " \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    " \\vdots\\\\\n",
    " \\frac{\\partial l}{\\partial x_{n}}\n",
    " \\end{array}\\right)\\end{split}$$\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 Tensor\n",
    "\n",
    "### 2.1.1 ```Tensor``` class\n",
    "\n",
    "* ``` requires_grad ```\n",
    " * ```torch.Tensor``` class has an attribute ```.requires_grad```\n",
    " * If you set its ```.requires_grad``` as ```True```, **it starts to track all operations on it**.\n",
    " * defaults to ```False```.\n",
    " * ```requires_grad``` is contagious. It means after applying some tensor operator, the result also has ```requires_grad=True```.\n",
    "* ```.backward()```\n",
    " * ```.backward()``` can be used when you finish your computation to have all the gradients computed automatically. \n",
    "* ```.grad``` attribute.\n",
    " *  The gradient for this tensor will be accumulated into ```.grad``` attribute.\n",
    "* ```.detach()```\n",
    " * To stop a tensor from tracking history, you can call ```.detach()``` to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "* ```with torch.no_grad():```\n",
    " * To prevent tracking history (and using memory), you can also wrap the code block in ```with torch.no_grad():```.\n",
    " * Can be used when evaluating a model with some trainable parameters(```.requires_grad=True```), but we don’t need the gradients.\n",
    "\n",
    "### 2.1.2 ```Function``` class\n",
    "\n",
    "```Tensor``` and ```Function``` are interconnected and build up an acyclic graph, that encodes a complete history of computation.\n",
    "\n",
    "* ```.grad_fn``` attribute\n",
    "\n",
    " * Each tensor has a ```.grad_fn``` attribute that references a Function that has created the ```Tensor``` \n",
    " * ```grad_fn``` is None if ```requires_grad``` is set to ```False```. \n",
    "\n",
    "## 2.1.3 Code\n",
    "\n",
    "Create a tensor and set ```requires_grad=True``` to track computation with it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "b88qUIc9wlwl",
    "outputId": "917f373a-13da-4554-e6e5-cf7cf858f781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-m8PcBWw0r2"
   },
   "source": [
    "Do a tensor operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "DMJ6Zu8Ow1Ls",
    "outputId": "ce06743e-7c6d-4a7e-b85f-a0e275db6adf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>) \n",
      "\n",
      "<AddBackward0 object at 0x7ff8a74bf048>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y,\"\\n\")\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iE28BeLmw2xA"
   },
   "source": [
    "```y``` was created as a result of an operation, so it has a ```grad_fn```.\n",
    "\n",
    "Do more operations on y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "U0BFLVAIw25V",
    "outputId": "21946c80-e76b-44ac-d799-1bd805d56275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) \n",
      "\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z,\"\\n\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X0fPaizbx_zZ"
   },
   "source": [
    "We can use ```.requires_grad_( ... )``` to change an existing Tensor’s ```requires_grad``` flag in-place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "2AFTLfN9yL-V",
    "outputId": "c5f6db3f-d307-4982-80db-bd656307d1bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True \n",
      "\n",
      "True\n",
      "<SumBackward0 object at 0x7ff8a74c5860>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad,\"\\n\")\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dXf-6dXU-cBg"
   },
   "source": [
    "## 2.2 Gradients\n",
    " \n",
    "Now we can do backprop:\n",
    "\n",
    "Because ``out`` contains a single scalar, ``out.backward()`` is\n",
    "equivalent to ``out.backward(torch.tensor(1))``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "hLhrF670-7ZB",
    "outputId": "f6033d19-8613-4ed6-9112-96bc00946232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "# out.backward(torch.tensor(1.))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWQ0hr45LcuG"
   },
   "source": [
    "We have that $out = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "Therefore,\n",
    "$\\frac{\\partial out}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
    "$\\frac{\\partial out}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n",
    "If the output is no longer a scalear, ```torch.autograd``` could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "jXExYSWOWfuH",
    "outputId": "ec3f5db4-041b-45bc-9e6f-236e6c3d065c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1388.3756, -279.0660, -226.1103], grad_fn=<MulBackward0>)\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)\n",
    "\n",
    "z = torch.randn(3, requires_grad=True)\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "Hbw7zOvwLw1y",
    "outputId": "5f2ae837-6eac-44d7-d17a-98f88ef866c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7dDojYkD8LK"
   },
   "source": [
    "Or by using ```.detach()``` to get a new Tensor with the same content but that does not require gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "zq7XUs3KTZyU",
    "outputId": "5700df67-2505-48d7-fa7f-18a9c324c811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all()) # If tensor x and y have same value"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "自动微分.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
