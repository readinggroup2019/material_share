{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**doc file** https://pytorch.org/docs/stable/torch.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5, 3)\n",
    "\n",
    "# a random matrix \n",
    "x = torch.rand(5, 3)\n",
    "# zero matrix\n",
    "x = torch.zeros(5, 3, dtype=torch.long)\n",
    "# construct a tensor from data\n",
    "x = torch.tensor([5.5, 3])\n",
    "# copy the size\n",
    "x = x.new_ones(5, 3, dtype=torch.double)      # new_* methods take in sizes\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4133, -1.4118, -0.9084],\n",
       "        [ 0.5822, -0.5884,  0.1504],\n",
       "        [-1.7354, -1.7094, -0.0556],\n",
       "        [ 0.6729,  1.1431, -2.8393],\n",
       "        [-0.6621, -4.6501, -1.9688]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oprations\n",
    "y=torch.randn(5,3)\n",
    "torch.add(x, y)\n",
    "# save an empty result\n",
    "result = torch.empty(5, 3)\n",
    "torch.add(x, y, out=result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0980,  0.7095,  0.1127,  0.4104, -2.6791])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oprations with _ will change the origin value\n",
    "# adds x to y\n",
    "y.add_(x)\n",
    "# choose the second column\n",
    "x[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5147828459739685\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "# change a one dimension tensor to a scaler\n",
    "x = torch.randn(1)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5147829]\n",
      "tensor([2.5148])\n",
      "[2.514783]\n"
     ]
    }
   ],
   "source": [
    "# change to numpy \n",
    "b = x.numpy()\n",
    "print(b)\n",
    "x.add_(1)\n",
    "print(x)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Autogrand\n",
    "\n",
    "`torch.Tensor` is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call `.backward()` and have all the gradients computed automatically. The gradient for this tensor will be accumulated into `.grad` attribute.\n",
    "\n",
    "To stop a tensor from tracking history, you can call `.detach()` to detach it from the computation history, and to prevent future computation from being tracked.\n",
    "\n",
    "To prevent tracking history (and using memory), you can also wrap the code block in` with torch.no_grad():`. This can be particularly helpful when evaluating a model because the model may have trainable parameters with `requires_grad=True`, but for which we don’t need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd implementation - a Function.\n",
    "\n",
    "Tensor and Function are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor (except for Tensors created by the user - their `grad_fn` is None).\n",
    "\n",
    "If you want to compute the derivatives, you can call `.backward()` on a Tensor. If Tensor is a scalar (i.e. it holds a one element data), you don’t need to specify any arguments to `backward()`, however if it has more elements, you need to specify a gradient argument that is a tensor of matching shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "<AddBackward0 object at 0x11b3839d0>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3196,  1.2596],\n",
       "        [-0.7662, -0.1270]], requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y=x+2\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z,out)\n",
    "\n",
    "a=torch.randn(2,2)\n",
    "a.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "x.detach_()\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Neural Network\n",
    "\n",
    "An `nn.Module` contains layers, and a method `forward(input)`that returns the `output`.\n",
    "\n",
    "\n",
    "* Define the neural network that has some learnable parameters (or weights)\n",
    "* Iterate over a dataset of inputs\n",
    "* Process input through the network\n",
    "* Compute the loss (how far is the output from being correct)\n",
    "* Propagate gradients back into the network’s parameters\n",
    "* Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1027,  0.0981, -0.0164,  0.0369, -0.0228,  0.0953,  0.0892,  0.0306,\n",
      "         -0.0220, -0.0335]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "A loss function takes the (output, target) pair of inputs, and computes a value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different loss functions under the nn package . A simple loss is: `nn.MSELoss` which computes the mean-squared error between the input and the target.\n",
    "\n",
    "* L1Loss\n",
    "* MSELoss\n",
    "* KLDivLoss\n",
    "* BCELoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6608, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x11b3bb6d0>\n",
      "<AddmmBackward object at 0x11b3bb850>\n",
      "<AccumulateGrad object at 0x11b3bb6d0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back propagation\n",
    "\n",
    "To backpropagate the error all we have to do is to `loss.backward()`. You need to clear the existing gradients though, else gradients will be accumulated to existing gradients.\n",
    "\n",
    "Now we shall call `loss.backward()`, and have a look at conv1’s bias gradients before and after the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0179,  0.0067,  0.0049,  0.0167,  0.0039, -0.0015])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the weights\n",
    "\n",
    "`weight = weight - learning_rate * gradient`\n",
    "\n",
    "However, as you use neural networks, you want to use various different update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable this, we built a small package: torch.optim that implements all these methods. Using it is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier\n",
    "\n",
    "We have created a package called `torchvision`, that has data loaders for common datasets such as Imagenet, CIFAR10, MNIST, etc. and data transformers for images, `viz.`, `torchvision.datasets` and `torch.utils.data.DataLoader`.\n",
    "\n",
    "We will do the following steps in order:\n",
    "\n",
    "* Load and normalizing the CIFAR10 training and test datasets using torchvision\n",
    "* Define a Convolutional Neural Network\n",
    "* Define a loss function\n",
    "* Train the network on the training data\n",
    "* Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The neural network model with minist data\n",
    "by Chuting Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist =fetch_openml('mnist_784', version=1)\n",
    "X, y = mnist[\"data\"], mnist[\"target\"]\n",
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.utils.data import Dataset, TensorDataset\n",
    "np.random.seed(42)\n",
    "x_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).long()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [60000, 10000])\n",
    "\n",
    "trainloader = DataLoader(dataset=train_dataset, batch_size=4,shuffle=True,num_workers=2)\n",
    "valloader = DataLoader(dataset=val_dataset, batch_size=4,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the linear and relu layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigNet, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 120)\n",
    "        self.fc3 = nn.Linear(120, 84)\n",
    "        self.fc4 = nn.Linear(84, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "signet=SigNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[[-0.0120,  0.0448,  0.0334, -0.1033, -0.0431],\n",
      "          [-0.0649,  0.0358, -0.0696, -0.0167,  0.0846],\n",
      "          [ 0.0220,  0.0765, -0.1153,  0.0426,  0.1007],\n",
      "          [-0.1134,  0.0467, -0.1100, -0.0743,  0.0284],\n",
      "          [ 0.0071, -0.0526,  0.0121,  0.0971, -0.0761]],\n",
      "\n",
      "         [[ 0.0255,  0.0355, -0.0177, -0.0192,  0.0592],\n",
      "          [-0.0691, -0.0393,  0.0184,  0.0837, -0.0973],\n",
      "          [ 0.0891,  0.0466,  0.0521,  0.0464, -0.1138],\n",
      "          [-0.0576, -0.1098, -0.0133,  0.0197,  0.0296],\n",
      "          [-0.0905, -0.0232,  0.0572, -0.1115,  0.0074]],\n",
      "\n",
      "         [[ 0.0449,  0.0641,  0.0302, -0.0630,  0.0183],\n",
      "          [-0.0409,  0.0590, -0.0385, -0.0982, -0.0130],\n",
      "          [-0.0407, -0.0816, -0.0539,  0.0187,  0.0676],\n",
      "          [-0.1017, -0.0912,  0.0482, -0.1133, -0.0064],\n",
      "          [ 0.0978, -0.0718, -0.0459,  0.0039, -0.0425]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0876,  0.0997,  0.0443,  0.0014,  0.0644],\n",
      "          [-0.0412,  0.0191, -0.0201,  0.0687, -0.0532],\n",
      "          [-0.0598,  0.0071, -0.0527,  0.0380, -0.0835],\n",
      "          [-0.1065,  0.0917,  0.0519,  0.1026, -0.0605],\n",
      "          [-0.0433, -0.0100, -0.1119,  0.0871, -0.0699]],\n",
      "\n",
      "         [[ 0.0973,  0.0308,  0.0288, -0.0882, -0.0914],\n",
      "          [-0.0233,  0.0899, -0.0569, -0.0486, -0.0711],\n",
      "          [ 0.0251,  0.0996, -0.0399, -0.0426, -0.0549],\n",
      "          [ 0.0635,  0.0257,  0.1060,  0.0786, -0.0875],\n",
      "          [ 0.0267, -0.0481,  0.0796,  0.0088,  0.0946]],\n",
      "\n",
      "         [[ 0.0125, -0.0127, -0.0834, -0.0777, -0.0867],\n",
      "          [ 0.0910,  0.0076,  0.0266,  0.0534, -0.1024],\n",
      "          [ 0.0482,  0.0198, -0.0727, -0.0461, -0.0413],\n",
      "          [-0.0135, -0.0982, -0.0559,  0.0469, -0.0132],\n",
      "          [ 0.1097, -0.0418,  0.0375,  0.0730, -0.0884]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0158,  0.0916,  0.0744,  0.0804, -0.0222],\n",
      "          [-0.0672,  0.0246, -0.1136, -0.0347,  0.0896],\n",
      "          [ 0.0796,  0.0264, -0.0388, -0.0301,  0.0686],\n",
      "          [-0.0519, -0.0628,  0.0562,  0.1078,  0.0864],\n",
      "          [-0.0413,  0.0481, -0.0788,  0.0160,  0.0767]],\n",
      "\n",
      "         [[ 0.0789,  0.0519, -0.1044,  0.0569, -0.0359],\n",
      "          [-0.0192, -0.0824, -0.0129, -0.0881, -0.0368],\n",
      "          [-0.0836,  0.0684,  0.1131, -0.0012, -0.1062],\n",
      "          [-0.0242,  0.0601, -0.0040, -0.1062,  0.1124],\n",
      "          [-0.0480, -0.0469,  0.0804,  0.0245,  0.0390]],\n",
      "\n",
      "         [[ 0.0884,  0.0178, -0.0254, -0.0747,  0.0137],\n",
      "          [-0.0217,  0.0480,  0.0937,  0.0060, -0.0403],\n",
      "          [-0.0319, -0.1036, -0.1111, -0.0251,  0.0250],\n",
      "          [ 0.0574, -0.0029,  0.0771, -0.0102, -0.0542],\n",
      "          [ 0.0807, -0.0970,  0.1128, -0.0983, -0.0116]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0678, -0.0635, -0.0247, -0.0832, -0.0653],\n",
      "          [-0.0947, -0.0821, -0.0529,  0.0190,  0.0334],\n",
      "          [ 0.1017, -0.1041, -0.0610, -0.0254,  0.0655],\n",
      "          [-0.0882,  0.0291,  0.0517,  0.0517,  0.0171],\n",
      "          [ 0.0626,  0.0149,  0.0853,  0.0664, -0.1080]],\n",
      "\n",
      "         [[-0.1154, -0.0699,  0.0490, -0.0963, -0.0177],\n",
      "          [-0.1031, -0.0026, -0.0574, -0.0611,  0.1082],\n",
      "          [ 0.0911,  0.0278,  0.1125,  0.0230,  0.0781],\n",
      "          [ 0.0548,  0.1073,  0.0512,  0.0992, -0.0540],\n",
      "          [-0.0936,  0.0305, -0.0148, -0.0058, -0.0226]],\n",
      "\n",
      "         [[-0.0215, -0.1091,  0.0829,  0.0215, -0.0597],\n",
      "          [-0.0073, -0.1079,  0.0967,  0.1082,  0.0287],\n",
      "          [-0.0800, -0.0482, -0.1035, -0.1154,  0.1000],\n",
      "          [ 0.0625,  0.0865, -0.1113, -0.0240, -0.0815],\n",
      "          [-0.0466, -0.1111,  0.0063, -0.0876,  0.0113]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0261,  0.1029, -0.0658,  0.0822, -0.0480],\n",
      "          [-0.0288,  0.0593, -0.0190, -0.0272,  0.1097],\n",
      "          [ 0.0087, -0.0791,  0.0499, -0.0028,  0.0751],\n",
      "          [ 0.0123, -0.0375,  0.0391,  0.0319,  0.1052],\n",
      "          [ 0.1119, -0.0484,  0.0491,  0.0622, -0.0563]],\n",
      "\n",
      "         [[ 0.0399, -0.0189,  0.0678, -0.0703,  0.0115],\n",
      "          [-0.1101, -0.0624,  0.0346, -0.0988, -0.0109],\n",
      "          [-0.0226,  0.0617, -0.0643, -0.0453, -0.0387],\n",
      "          [ 0.0756, -0.1125,  0.0987, -0.0311,  0.0830],\n",
      "          [ 0.0649,  0.1023, -0.0975, -0.0960, -0.1132]],\n",
      "\n",
      "         [[ 0.0652, -0.0638,  0.0744, -0.0245, -0.0980],\n",
      "          [ 0.1053,  0.1049, -0.0606,  0.0203,  0.0087],\n",
      "          [ 0.1098,  0.0019, -0.0650,  0.0342, -0.0327],\n",
      "          [ 0.0578,  0.0333, -0.0334,  0.0748,  0.0802],\n",
      "          [-0.0546,  0.0834,  0.1039,  0.0218,  0.0871]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1145, -0.0731,  0.0986, -0.0928, -0.0894],\n",
      "          [ 0.0590, -0.0865, -0.1105,  0.0350,  0.0165],\n",
      "          [ 0.0729,  0.0915, -0.0337, -0.0335, -0.0272],\n",
      "          [-0.0347,  0.0237, -0.0516,  0.0942, -0.0974],\n",
      "          [-0.0289, -0.0715, -0.0473,  0.0884,  0.0892]],\n",
      "\n",
      "         [[ 0.0859, -0.0582,  0.0138,  0.0784, -0.0108],\n",
      "          [ 0.0672, -0.0004,  0.0721,  0.0301,  0.0783],\n",
      "          [ 0.0493, -0.0510, -0.0224,  0.0424,  0.0352],\n",
      "          [ 0.0767,  0.0983, -0.1008, -0.0012,  0.0151],\n",
      "          [-0.1101,  0.1073, -0.0057, -0.1041,  0.0015]],\n",
      "\n",
      "         [[-0.1037, -0.0641, -0.0493, -0.0600,  0.0491],\n",
      "          [ 0.1144,  0.1154,  0.1054, -0.1108, -0.0361],\n",
      "          [ 0.0020,  0.0320,  0.0247,  0.0545, -0.0332],\n",
      "          [-0.0119,  0.0296,  0.0788, -0.0436, -0.0892],\n",
      "          [ 0.1113,  0.1142, -0.0078,  0.0260,  0.0705]]]])), ('conv1.bias', tensor([-0.0188,  0.0615,  0.0507,  0.0921, -0.0618,  0.0798])), ('conv2.weight', tensor([[[[ 6.6671e-02,  5.1262e-02,  1.0030e-02,  7.2301e-02,  1.8002e-02],\n",
      "          [ 1.1716e-02, -5.0679e-02,  1.8109e-03,  6.0756e-02, -6.9232e-02],\n",
      "          [-9.9832e-03, -1.7355e-02, -2.5346e-02,  5.0028e-03, -3.0426e-02],\n",
      "          [-9.1323e-03, -8.0791e-02, -6.6848e-02, -3.8370e-02, -6.6306e-02],\n",
      "          [-5.7737e-02,  7.9837e-03, -5.0449e-02,  1.0830e-02,  6.6129e-02]],\n",
      "\n",
      "         [[-5.4382e-03, -7.1538e-02,  5.0468e-04, -6.3132e-02, -4.5441e-03],\n",
      "          [-3.3830e-02, -1.5233e-02, -6.4004e-02, -4.6952e-02,  4.4818e-02],\n",
      "          [-3.4011e-02,  6.9188e-02,  1.9993e-02, -7.2312e-02, -3.7712e-02],\n",
      "          [ 1.7541e-02, -5.9320e-02,  6.3349e-03,  2.2038e-02, -3.1691e-03],\n",
      "          [ 2.7334e-02,  5.9115e-02, -3.0925e-02,  7.7333e-02,  2.7134e-03]],\n",
      "\n",
      "         [[ 4.1799e-02, -7.2825e-02, -7.0286e-02,  5.6261e-02, -2.0236e-03],\n",
      "          [-5.5196e-02, -9.2951e-03,  2.2251e-02,  6.2483e-02,  7.8497e-03],\n",
      "          [-5.9445e-02,  3.4286e-02,  1.4518e-02,  4.5373e-02, -9.8336e-03],\n",
      "          [-4.9353e-02, -2.4889e-02,  7.7155e-02, -6.4164e-02,  2.2241e-02],\n",
      "          [ 7.5062e-02,  5.3309e-02,  1.4780e-02, -9.1410e-03, -6.3680e-02]],\n",
      "\n",
      "         [[ 5.9010e-02,  7.1917e-02,  5.6453e-02,  7.4032e-02, -3.2790e-02],\n",
      "          [-2.4018e-02, -4.0266e-02, -3.5107e-02, -2.6819e-02,  3.1213e-02],\n",
      "          [-6.8253e-03,  5.0948e-02, -1.0998e-02, -5.4964e-03, -3.7429e-03],\n",
      "          [-2.9728e-02, -5.9844e-02, -2.5906e-02,  5.2382e-02,  1.9889e-02],\n",
      "          [ 4.4253e-02,  7.4417e-02,  7.3688e-02,  2.2636e-02,  6.5939e-02]],\n",
      "\n",
      "         [[ 6.5196e-02,  1.2275e-02,  6.6232e-02, -6.8249e-02, -5.7539e-02],\n",
      "          [ 7.4898e-02,  6.2561e-02, -7.1268e-02, -8.1430e-02,  2.9601e-02],\n",
      "          [-3.9252e-02,  3.5873e-02,  7.7624e-03, -6.5619e-02, -1.0493e-02],\n",
      "          [-8.0868e-02,  3.8024e-02, -7.0042e-02,  7.4562e-02, -4.3392e-02],\n",
      "          [ 5.8678e-03,  3.5434e-02,  3.6840e-02,  1.4101e-02,  4.4973e-02]],\n",
      "\n",
      "         [[-2.7667e-02,  5.7505e-02,  4.2949e-02,  4.6200e-03, -6.4661e-03],\n",
      "          [ 1.7086e-02, -9.1683e-03, -8.0049e-02, -8.2712e-03,  1.6237e-02],\n",
      "          [ 3.7241e-02,  1.9995e-03,  2.6774e-02,  7.8076e-03, -5.5413e-02],\n",
      "          [ 8.0992e-02, -1.2539e-02,  6.4851e-02, -7.9507e-02, -2.1424e-02],\n",
      "          [ 2.2599e-02, -6.5117e-02, -3.5760e-02, -6.3167e-02, -3.7304e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.6251e-02,  7.1636e-02,  3.9142e-02,  4.0091e-02, -4.9603e-02],\n",
      "          [ 5.4202e-02,  3.3621e-02,  5.4238e-02, -3.1111e-02, -7.2099e-03],\n",
      "          [-6.7718e-02,  2.6470e-02, -4.1631e-02, -3.0779e-02, -5.6471e-02],\n",
      "          [-7.6951e-02, -7.6318e-02, -7.4690e-02,  2.8357e-02, -2.4319e-02],\n",
      "          [ 5.2975e-02,  6.2687e-02,  7.6797e-02,  2.1163e-02,  7.5685e-02]],\n",
      "\n",
      "         [[ 7.1431e-02,  2.0791e-02, -1.3631e-02, -2.3757e-02,  3.7226e-02],\n",
      "          [-3.7304e-02, -6.4529e-02, -3.5922e-02,  6.3506e-02, -7.5064e-02],\n",
      "          [-5.1520e-03,  2.0078e-02, -3.4016e-02,  1.9075e-03, -1.5082e-02],\n",
      "          [ 7.4277e-02,  2.5036e-02,  5.9689e-02,  7.2858e-02,  7.1722e-02],\n",
      "          [-7.7776e-02, -1.9590e-02, -3.6990e-02,  7.9055e-02, -7.2892e-02]],\n",
      "\n",
      "         [[-5.0221e-02, -4.2807e-02,  4.0790e-02, -7.2388e-02,  5.6218e-02],\n",
      "          [ 7.4621e-02,  7.4101e-02, -6.7569e-02,  3.9438e-02, -3.6245e-02],\n",
      "          [ 7.4858e-02, -4.0937e-02, -3.3835e-02,  4.1271e-02,  2.9927e-02],\n",
      "          [-6.7290e-02,  5.7577e-03, -3.1954e-03, -7.5301e-02,  6.1036e-02],\n",
      "          [ 4.8173e-03, -3.3527e-02, -7.4631e-02, -4.2270e-02,  4.5530e-02]],\n",
      "\n",
      "         [[-8.0717e-02, -3.2255e-02, -3.6917e-02, -6.3934e-03, -5.0902e-02],\n",
      "          [-4.9459e-02,  7.1639e-02,  1.0513e-02, -6.8241e-02,  6.8540e-03],\n",
      "          [-6.3500e-02, -6.7948e-02,  3.7321e-02, -5.7669e-02, -1.2899e-02],\n",
      "          [-3.4126e-02,  7.3243e-02,  2.1490e-02,  3.3188e-02,  4.5959e-02],\n",
      "          [-7.2677e-02,  1.4606e-02,  5.5041e-02, -6.6318e-02,  7.1583e-02]],\n",
      "\n",
      "         [[ 9.3434e-03,  6.6684e-02, -7.9900e-02,  7.0085e-02, -4.5495e-02],\n",
      "          [ 1.9402e-02,  5.8530e-02,  5.9831e-03,  3.2701e-02, -2.5635e-02],\n",
      "          [-7.6155e-02,  2.3613e-02, -7.9745e-02,  1.3183e-02,  3.4432e-02],\n",
      "          [ 7.2456e-02,  7.9538e-02,  5.4801e-02, -5.6870e-02,  4.5965e-02],\n",
      "          [-1.3066e-03,  7.7917e-02,  5.3455e-02, -6.1516e-02, -5.5831e-02]],\n",
      "\n",
      "         [[-6.9949e-02,  6.4077e-02, -3.0446e-02, -5.1408e-02,  5.6440e-02],\n",
      "          [-5.9048e-02, -5.5827e-02, -8.9695e-04,  2.1836e-02,  4.3874e-02],\n",
      "          [-4.6439e-02, -3.8316e-02, -8.6378e-03, -1.7365e-02, -1.1482e-02],\n",
      "          [ 2.5888e-02,  4.9600e-02, -6.9296e-02, -6.1678e-02, -6.9527e-02],\n",
      "          [-4.5204e-02,  1.3431e-02, -1.2864e-03,  5.0800e-02, -5.0051e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9873e-02,  2.1558e-02, -1.3672e-03, -6.3909e-02, -4.5950e-02],\n",
      "          [ 3.9236e-02,  2.4043e-02, -2.7926e-02, -3.8949e-02,  2.8904e-02],\n",
      "          [-7.4291e-02, -1.8087e-02, -4.5338e-02,  7.5581e-02,  6.9563e-02],\n",
      "          [ 4.8332e-02, -5.0878e-02, -1.1997e-02, -3.4634e-02,  1.6313e-02],\n",
      "          [-2.0825e-02,  4.1224e-02, -6.2770e-02,  5.9790e-03,  6.1654e-02]],\n",
      "\n",
      "         [[ 1.6626e-02,  5.6980e-03, -2.9895e-02, -5.3436e-02,  5.2881e-02],\n",
      "          [ 4.4515e-02, -1.4897e-02, -3.1133e-03, -1.1215e-02, -6.1865e-02],\n",
      "          [ 5.1327e-03, -4.9368e-02,  8.1214e-02, -5.7891e-03, -3.1491e-04],\n",
      "          [ 7.3791e-02, -6.0764e-02,  3.8480e-02, -2.2216e-02, -3.2917e-02],\n",
      "          [-4.9223e-02,  9.2132e-04,  6.1524e-02, -5.3911e-02,  4.7361e-02]],\n",
      "\n",
      "         [[ 7.1195e-02,  7.6625e-02,  6.8305e-02,  5.7100e-03, -6.2644e-03],\n",
      "          [ 3.6050e-02,  7.9115e-02, -5.0989e-02,  2.1883e-02,  1.0942e-02],\n",
      "          [ 4.9581e-02,  6.6435e-02, -5.8961e-02, -2.2364e-02,  8.0741e-02],\n",
      "          [ 2.6950e-02, -2.0921e-02,  1.8676e-02,  2.0031e-02,  7.8908e-02],\n",
      "          [ 2.3686e-02, -5.8858e-02, -2.2337e-02,  4.2954e-02, -2.2214e-02]],\n",
      "\n",
      "         [[ 2.7109e-02,  1.2783e-02,  6.5798e-03,  6.6850e-02, -7.7123e-02],\n",
      "          [ 2.5570e-02, -3.4167e-02, -6.1254e-02, -1.6554e-02,  1.9697e-02],\n",
      "          [-3.8303e-02, -1.9984e-02,  7.0877e-02, -1.2909e-02, -4.5911e-02],\n",
      "          [-1.3902e-02,  7.5413e-02, -4.5249e-02, -2.0280e-03, -1.6984e-03],\n",
      "          [-6.7290e-02,  6.1864e-03,  3.3565e-02,  1.5881e-02,  1.7030e-02]],\n",
      "\n",
      "         [[ 3.0406e-02,  6.6171e-02, -2.4412e-02,  4.1611e-02, -5.5135e-02],\n",
      "          [-6.1311e-02, -1.6897e-02, -3.8522e-03, -4.3576e-02, -5.2461e-02],\n",
      "          [ 4.2372e-02,  4.2459e-03, -7.6982e-02, -9.5435e-03,  2.7942e-02],\n",
      "          [-3.8446e-02, -6.4580e-02, -2.1672e-04,  1.3583e-02, -6.9573e-02],\n",
      "          [-1.9990e-02,  5.7939e-02,  2.9312e-02,  4.6019e-02, -2.5235e-02]],\n",
      "\n",
      "         [[ 3.9430e-02,  5.5710e-02, -6.1876e-02,  5.5743e-02,  2.5347e-02],\n",
      "          [ 3.3672e-02,  7.4036e-02,  9.3409e-03,  6.4791e-02,  6.5679e-02],\n",
      "          [ 6.1583e-02, -5.4946e-02, -7.2631e-02,  4.5739e-02,  4.8814e-03],\n",
      "          [ 1.2294e-02,  5.6767e-02, -1.1460e-02, -6.5374e-02,  1.9129e-02],\n",
      "          [ 6.6083e-02,  4.7457e-02, -4.1729e-02, -1.6165e-02, -2.5195e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.2439e-02,  1.9287e-02, -2.9536e-02,  4.2296e-02, -4.8781e-02],\n",
      "          [ 2.1453e-02,  4.9654e-02, -2.4889e-02, -5.2494e-02, -6.8248e-02],\n",
      "          [ 2.5706e-02, -2.0080e-02, -7.7481e-02, -3.4890e-02, -3.2495e-02],\n",
      "          [ 2.2445e-02, -7.9679e-02,  7.1535e-03,  4.1440e-02,  1.6650e-02],\n",
      "          [ 5.9287e-02,  1.6247e-02, -2.6931e-02, -5.6417e-04,  1.9338e-02]],\n",
      "\n",
      "         [[ 7.0485e-02,  6.3063e-02,  2.9503e-02, -4.3147e-02,  7.3099e-02],\n",
      "          [ 5.6644e-02, -8.4298e-04, -5.8451e-03, -2.0540e-02, -3.6725e-02],\n",
      "          [-7.2709e-02,  3.4895e-02,  3.6835e-02,  1.4723e-02, -8.0783e-02],\n",
      "          [ 8.2772e-03, -5.7701e-02,  5.6292e-02, -5.1314e-02, -4.9434e-02],\n",
      "          [ 4.6032e-02,  7.5754e-03, -7.4932e-02, -1.9851e-02,  4.6362e-03]],\n",
      "\n",
      "         [[ 2.8030e-03,  5.3485e-02,  1.1091e-02, -9.2996e-03,  7.2831e-02],\n",
      "          [ 1.8690e-02,  4.2797e-02,  4.6956e-02,  4.1257e-02,  3.9456e-02],\n",
      "          [-7.8205e-02,  1.9964e-02,  3.1393e-02, -7.3787e-02, -7.8060e-05],\n",
      "          [-6.7655e-02,  1.0289e-02,  2.1641e-02, -6.6512e-03,  5.0489e-02],\n",
      "          [-2.3974e-02,  3.9534e-02,  4.8345e-02, -5.3253e-02,  1.7604e-02]],\n",
      "\n",
      "         [[-2.9914e-02,  4.3605e-02, -6.4023e-02,  4.3220e-02,  6.2312e-02],\n",
      "          [ 1.5200e-02,  4.4868e-02, -7.8974e-02,  5.7030e-02,  7.5679e-02],\n",
      "          [-1.3781e-02, -4.8193e-02, -2.0968e-03,  5.2736e-02, -6.6455e-02],\n",
      "          [ 2.5565e-02, -6.9679e-02,  3.0190e-04,  1.5116e-02,  6.3322e-02],\n",
      "          [ 2.1960e-02,  2.0680e-02,  4.4428e-02, -6.2419e-02,  2.4348e-02]],\n",
      "\n",
      "         [[-6.0165e-04, -7.5904e-02, -4.8048e-02,  5.6536e-02, -5.3199e-03],\n",
      "          [-3.6441e-02, -3.9073e-02,  6.6945e-02,  5.0636e-02,  1.7885e-02],\n",
      "          [-3.7338e-02,  1.5916e-02,  6.8799e-02, -6.4067e-02, -3.9676e-02],\n",
      "          [ 1.5411e-02,  4.9178e-03, -6.5369e-02, -5.0109e-02, -1.2913e-02],\n",
      "          [-4.6837e-02, -7.5833e-02, -4.5325e-02,  5.8587e-02, -5.2363e-02]],\n",
      "\n",
      "         [[ 4.2763e-02,  7.2805e-02, -7.3904e-02,  3.9030e-02, -6.9201e-02],\n",
      "          [ 7.0829e-02,  8.1560e-02, -1.0433e-02, -4.0446e-02,  6.1049e-02],\n",
      "          [-6.7828e-02,  2.4128e-02,  6.6232e-03, -8.1565e-02,  1.3501e-03],\n",
      "          [-6.9036e-04,  6.2496e-02, -2.2294e-02,  4.1507e-02, -6.4334e-02],\n",
      "          [-2.4053e-02,  4.1250e-02, -2.3466e-02, -7.0487e-02, -3.0689e-02]]],\n",
      "\n",
      "\n",
      "        [[[-4.1770e-02, -7.7647e-02,  3.1778e-03,  2.6460e-02, -1.8424e-02],\n",
      "          [ 5.8611e-02,  5.5326e-02,  5.1681e-02, -6.7589e-03,  1.7748e-02],\n",
      "          [ 3.6999e-02,  1.5651e-02,  7.6986e-02, -2.5279e-02, -3.5828e-02],\n",
      "          [ 6.4782e-02, -6.0903e-02, -5.0622e-02,  7.6001e-04,  3.2145e-02],\n",
      "          [-7.9926e-02, -1.9147e-02, -5.0915e-02,  6.4916e-02,  5.4218e-02]],\n",
      "\n",
      "         [[-4.0138e-02,  3.5458e-02,  9.6403e-05,  2.4360e-02,  4.8790e-02],\n",
      "          [-8.1316e-02, -2.0531e-02, -4.6868e-02,  1.7685e-02, -2.5056e-02],\n",
      "          [-6.4582e-02, -2.4790e-02,  5.9333e-02,  7.4121e-02, -2.1657e-02],\n",
      "          [-6.5837e-03, -1.1325e-02,  5.8589e-02,  5.4676e-02, -7.2909e-02],\n",
      "          [ 6.5122e-02, -6.2600e-02,  3.8025e-02,  7.3460e-02,  6.1565e-02]],\n",
      "\n",
      "         [[-2.9026e-03, -6.4551e-02, -3.7171e-02,  7.5477e-02, -7.4158e-03],\n",
      "          [ 3.2333e-02, -6.6745e-02, -6.9209e-02,  5.3390e-02,  4.8542e-02],\n",
      "          [-3.5533e-02,  4.6576e-02,  4.5022e-02,  4.8854e-02, -2.2047e-02],\n",
      "          [-1.8239e-02, -7.7208e-02,  1.8524e-02, -4.6103e-02,  2.0731e-02],\n",
      "          [ 8.0413e-02,  1.3100e-02,  6.1252e-02, -2.9266e-02, -2.3960e-02]],\n",
      "\n",
      "         [[-3.3769e-02,  5.4560e-02, -2.6990e-02, -7.5306e-02, -4.1256e-02],\n",
      "          [ 1.7755e-02, -4.5597e-02,  2.0806e-03,  6.3516e-02,  8.0252e-03],\n",
      "          [-1.8602e-02,  3.2355e-02,  3.1532e-02, -2.1594e-02, -4.7978e-02],\n",
      "          [-4.3358e-03,  4.0971e-02,  3.0820e-04,  7.1220e-02, -7.1516e-02],\n",
      "          [ 6.4906e-02, -2.6874e-02,  3.2482e-02, -5.9192e-02, -6.7381e-03]],\n",
      "\n",
      "         [[ 3.3866e-02,  5.3410e-02, -5.7358e-02, -3.9437e-03, -6.4723e-02],\n",
      "          [-5.3840e-02,  7.1387e-02,  7.3667e-02, -7.5824e-02,  2.5012e-02],\n",
      "          [-1.0401e-02,  2.8753e-02,  3.8178e-02, -6.3198e-02,  5.9328e-02],\n",
      "          [ 1.1720e-02,  5.2189e-02, -3.1789e-02,  2.6862e-02, -9.5020e-03],\n",
      "          [ 2.7216e-02,  3.1714e-02,  3.0896e-02, -4.4575e-02,  1.8293e-02]],\n",
      "\n",
      "         [[-7.0865e-03,  1.5873e-02, -2.5371e-02,  5.8581e-02,  7.1187e-02],\n",
      "          [-4.7242e-02, -6.1486e-02,  9.1989e-04, -5.3903e-02,  1.1197e-03],\n",
      "          [ 5.8655e-02, -8.0429e-02, -8.0367e-02, -5.1784e-02,  2.4117e-02],\n",
      "          [ 2.3988e-02, -3.8666e-02, -6.4790e-02,  6.5497e-02,  7.4808e-03],\n",
      "          [ 7.5723e-02, -2.2124e-02, -7.3318e-02,  1.8919e-02, -4.2786e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8192e-02, -1.2020e-02, -8.3677e-03, -6.8480e-02,  4.8544e-02],\n",
      "          [ 7.6086e-02, -4.4679e-02,  7.8585e-02,  5.0340e-03,  6.9534e-02],\n",
      "          [-2.9735e-02, -8.5935e-03, -7.4515e-02,  7.5276e-02,  1.5867e-02],\n",
      "          [ 7.0444e-02,  7.8079e-02,  6.5522e-02, -3.3774e-02, -6.6444e-02],\n",
      "          [-2.0165e-02, -5.4238e-02,  5.6091e-02,  2.5724e-02, -2.0097e-02]],\n",
      "\n",
      "         [[-6.9445e-02, -4.6769e-02, -2.4578e-02,  6.6510e-03,  3.7067e-02],\n",
      "          [ 5.5525e-02, -2.8434e-02,  5.4399e-02,  5.9407e-03,  1.8562e-02],\n",
      "          [-4.6559e-02,  8.0848e-02, -3.4685e-02, -4.2392e-02,  6.8502e-03],\n",
      "          [-8.5555e-03, -6.2870e-04, -6.4103e-02, -4.5118e-02, -3.5665e-02],\n",
      "          [-5.1372e-02,  7.4951e-02, -5.9793e-02, -2.4759e-03,  2.2735e-02]],\n",
      "\n",
      "         [[-6.9998e-02, -6.6578e-02, -3.0071e-02,  2.2276e-02,  1.9709e-02],\n",
      "          [ 9.6653e-04,  7.5268e-02, -7.1322e-02, -7.7058e-02,  1.9114e-02],\n",
      "          [-4.4393e-02,  6.5883e-02, -5.8885e-02, -5.9231e-02,  3.8075e-03],\n",
      "          [-3.1466e-02, -8.4408e-05,  9.7085e-03, -6.1972e-02,  7.8754e-02],\n",
      "          [ 2.2804e-02,  1.0912e-02, -7.3918e-03,  1.7962e-02,  1.4742e-02]],\n",
      "\n",
      "         [[-6.8419e-02,  1.2295e-03,  4.3028e-02,  7.5030e-02,  6.5644e-02],\n",
      "          [-7.8561e-02, -4.3095e-02, -2.5563e-02,  4.5168e-02, -3.0711e-02],\n",
      "          [-1.3947e-02, -1.3881e-02, -6.4291e-02,  7.6113e-02,  6.7094e-02],\n",
      "          [ 4.3826e-03,  2.2837e-02, -8.0811e-02,  5.1628e-02,  6.9370e-02],\n",
      "          [-2.0627e-02,  7.2067e-02, -5.8589e-02,  5.4034e-02,  7.6717e-02]],\n",
      "\n",
      "         [[ 7.6957e-02, -1.3720e-02, -3.3864e-03,  3.1930e-02,  1.6241e-02],\n",
      "          [ 6.0189e-02,  8.9397e-03,  5.0411e-02,  1.1176e-02,  7.0720e-02],\n",
      "          [-2.4239e-02, -2.4898e-02, -5.8207e-02, -6.2875e-02,  7.1925e-02],\n",
      "          [ 8.1490e-02, -6.7164e-02, -1.8036e-02, -3.9628e-02, -6.5408e-02],\n",
      "          [ 5.9742e-02, -2.5577e-02,  5.0195e-02,  4.6845e-03, -7.4594e-02]],\n",
      "\n",
      "         [[-2.6567e-02,  2.8904e-02, -1.2501e-02, -5.7074e-02,  6.8758e-02],\n",
      "          [-2.3483e-02,  1.1158e-02, -1.8679e-02, -5.2794e-02,  6.3259e-02],\n",
      "          [-4.3602e-02, -3.7095e-02, -3.2320e-02, -7.6264e-03,  6.3812e-02],\n",
      "          [ 7.2034e-02, -2.2207e-02, -4.9341e-02,  3.0202e-02,  3.2057e-02],\n",
      "          [-1.3503e-02, -1.6494e-02,  3.9800e-02,  1.8539e-02, -7.5500e-03]]]])), ('conv2.bias', tensor([ 0.0503,  0.0253, -0.0119, -0.0312,  0.0772, -0.0559, -0.0686, -0.0163,\n",
      "         0.0352, -0.0174, -0.0727, -0.0257,  0.0368,  0.0508, -0.0073, -0.0519])), ('fc1.weight', tensor([[ 0.0500,  0.0335, -0.0381,  ..., -0.0098,  0.0041,  0.0306],\n",
      "        [ 0.0424, -0.0494,  0.0073,  ...,  0.0197, -0.0134, -0.0148],\n",
      "        [ 0.0359, -0.0102,  0.0395,  ..., -0.0218, -0.0436,  0.0160],\n",
      "        ...,\n",
      "        [ 0.0064, -0.0285,  0.0193,  ...,  0.0102, -0.0182,  0.0155],\n",
      "        [-0.0382,  0.0167, -0.0285,  ..., -0.0355,  0.0294, -0.0103],\n",
      "        [-0.0130, -0.0462,  0.0498,  ...,  0.0128,  0.0051, -0.0220]])), ('fc1.bias', tensor([-0.0347, -0.0282,  0.0088, -0.0349,  0.0097,  0.0464, -0.0189,  0.0045,\n",
      "         0.0289, -0.0498, -0.0371,  0.0145,  0.0312, -0.0328,  0.0013,  0.0385,\n",
      "         0.0181,  0.0230,  0.0345, -0.0365,  0.0024, -0.0151,  0.0422, -0.0002,\n",
      "        -0.0459,  0.0265,  0.0362, -0.0085, -0.0287, -0.0383,  0.0388, -0.0390,\n",
      "         0.0400,  0.0206, -0.0039, -0.0164, -0.0402,  0.0225, -0.0146,  0.0474,\n",
      "         0.0269,  0.0333,  0.0450,  0.0325, -0.0177, -0.0420,  0.0016, -0.0069,\n",
      "         0.0398, -0.0223,  0.0238, -0.0111, -0.0043, -0.0469, -0.0483, -0.0252,\n",
      "         0.0433,  0.0129,  0.0071,  0.0218,  0.0436, -0.0398,  0.0327, -0.0314,\n",
      "        -0.0269,  0.0323, -0.0061, -0.0433, -0.0429,  0.0217, -0.0476,  0.0339,\n",
      "         0.0441,  0.0293, -0.0437,  0.0167, -0.0287, -0.0275,  0.0337,  0.0305,\n",
      "        -0.0238, -0.0098,  0.0323, -0.0355, -0.0277, -0.0436, -0.0444, -0.0205,\n",
      "        -0.0130,  0.0347, -0.0074, -0.0439, -0.0311,  0.0376,  0.0226, -0.0171,\n",
      "        -0.0193,  0.0383, -0.0237,  0.0030, -0.0466,  0.0269, -0.0003, -0.0227,\n",
      "         0.0011,  0.0353,  0.0051, -0.0444,  0.0091, -0.0168, -0.0185, -0.0429,\n",
      "        -0.0324, -0.0044, -0.0255, -0.0485, -0.0039, -0.0394,  0.0004, -0.0136])), ('fc2.weight', tensor([[-0.0771, -0.0452, -0.0227,  ...,  0.0415,  0.0320, -0.0388],\n",
      "        [ 0.0130, -0.0498, -0.0799,  ..., -0.0565,  0.0551, -0.0513],\n",
      "        [-0.0277, -0.0399,  0.0563,  ...,  0.0808,  0.0480,  0.0811],\n",
      "        ...,\n",
      "        [-0.0435, -0.0521, -0.0901,  ..., -0.0382,  0.0574, -0.0337],\n",
      "        [-0.0512, -0.0421, -0.0128,  ..., -0.0293,  0.0781, -0.0590],\n",
      "        [-0.0602,  0.0459,  0.0258,  ...,  0.0153,  0.0325,  0.0127]])), ('fc2.bias', tensor([-0.0708, -0.0480,  0.0271,  0.0694,  0.0622,  0.0890,  0.0198,  0.0651,\n",
      "         0.0414,  0.0751, -0.0123,  0.0211,  0.0708,  0.0233, -0.0062,  0.0083,\n",
      "        -0.0679,  0.0329, -0.0264,  0.0014, -0.0737,  0.0378,  0.0474,  0.0399,\n",
      "         0.0032,  0.0595, -0.0698, -0.0857,  0.0053,  0.0315,  0.0340, -0.0699,\n",
      "         0.0601, -0.0883, -0.0049, -0.0425, -0.0824,  0.0458,  0.0377,  0.0096,\n",
      "         0.0587, -0.0812,  0.0530, -0.0435,  0.0352, -0.0192,  0.0335,  0.0168,\n",
      "        -0.0437, -0.0804,  0.0061,  0.0047, -0.0757,  0.0856, -0.0522, -0.0106,\n",
      "         0.0692, -0.0665,  0.0729,  0.0471, -0.0049, -0.0352,  0.0698,  0.0457,\n",
      "         0.0538,  0.0777, -0.0149,  0.0641,  0.0780,  0.0246, -0.0082,  0.0103,\n",
      "        -0.0358,  0.0411,  0.0561,  0.0240,  0.0838, -0.0355,  0.0779,  0.0127,\n",
      "         0.0428,  0.0590, -0.0259,  0.0374])), ('fc3.weight', tensor([[-0.0491,  0.0070, -0.0329, -0.0679,  0.0385,  0.0932, -0.0422,  0.0105,\n",
      "         -0.0112,  0.0431,  0.0426,  0.0477, -0.0451,  0.0215,  0.0818,  0.0247,\n",
      "          0.0925,  0.0529,  0.0632,  0.0507,  0.0589,  0.0040,  0.0042,  0.0099,\n",
      "          0.0487, -0.0158, -0.0363,  0.0650,  0.0046, -0.0796, -0.0440,  0.0472,\n",
      "          0.0773, -0.0711, -0.0994, -0.0094, -0.0048,  0.0195, -0.0854, -0.0469,\n",
      "          0.0633,  0.0179, -0.0071, -0.0329, -0.0751, -0.0109, -0.0196, -0.0034,\n",
      "         -0.0336,  0.0201, -0.0587, -0.0927, -0.0222, -0.0199,  0.0592,  0.0356,\n",
      "          0.0101, -0.0624, -0.0611,  0.0584,  0.1072, -0.0924,  0.0493, -0.0605,\n",
      "          0.0288,  0.0916, -0.0127, -0.0051, -0.0181,  0.0190,  0.0905, -0.0572,\n",
      "          0.0863,  0.0450, -0.0347, -0.0855,  0.0712,  0.0579,  0.0915,  0.0054,\n",
      "          0.0358, -0.0837,  0.0070,  0.0629],\n",
      "        [ 0.1046, -0.0485, -0.0459, -0.0472,  0.0453,  0.0817,  0.0404, -0.0450,\n",
      "          0.0395,  0.0288,  0.0512,  0.0871, -0.0383,  0.0431,  0.0402, -0.1069,\n",
      "          0.0901,  0.0902, -0.0603,  0.0534,  0.0336, -0.1085, -0.0533, -0.0742,\n",
      "         -0.0878, -0.1045, -0.0505, -0.0513, -0.0039, -0.0242, -0.0970,  0.0341,\n",
      "         -0.0876,  0.0183, -0.0540, -0.0093,  0.0123,  0.0293, -0.0326, -0.0217,\n",
      "          0.0369,  0.0446,  0.0207,  0.0942,  0.0321, -0.0981,  0.1029, -0.0587,\n",
      "         -0.0257,  0.0033, -0.0219,  0.0932, -0.0298, -0.0212, -0.0277,  0.0846,\n",
      "         -0.0833,  0.0833, -0.0135, -0.1004, -0.0098, -0.0840, -0.1031, -0.0294,\n",
      "          0.0311,  0.0055,  0.0383, -0.0943,  0.0459, -0.0610,  0.0735, -0.0364,\n",
      "         -0.0117, -0.0882,  0.0501, -0.0483,  0.0261, -0.0849, -0.0497,  0.0473,\n",
      "         -0.0969, -0.0761, -0.0145,  0.1050],\n",
      "        [ 0.0369, -0.0309,  0.0167,  0.0283, -0.0880, -0.0853, -0.0219,  0.0141,\n",
      "          0.0390,  0.0788, -0.1009,  0.0187,  0.0499, -0.0755, -0.0267, -0.0196,\n",
      "          0.0352, -0.0066,  0.0971, -0.0730,  0.0783,  0.0928,  0.0166,  0.0284,\n",
      "         -0.0597,  0.0207, -0.0893, -0.0314, -0.1050, -0.0774,  0.1090,  0.0056,\n",
      "          0.0473,  0.0233,  0.0113,  0.0539,  0.0648,  0.0546,  0.0168, -0.0665,\n",
      "         -0.0513, -0.0365,  0.0340,  0.0984,  0.0257, -0.0279, -0.0319, -0.0057,\n",
      "         -0.0960,  0.0992,  0.0163,  0.0769, -0.0084, -0.0586,  0.0731,  0.0270,\n",
      "          0.0452, -0.0299,  0.0773,  0.0555, -0.0829,  0.0797,  0.0702, -0.0427,\n",
      "         -0.0122,  0.0719, -0.0399,  0.0337, -0.0543,  0.0765,  0.0098, -0.0678,\n",
      "         -0.0514, -0.0436, -0.0332, -0.0236,  0.0268, -0.1068, -0.0198,  0.0895,\n",
      "          0.0764,  0.0589,  0.0642,  0.0309],\n",
      "        [-0.0738,  0.0001,  0.0435,  0.0365, -0.0725,  0.0356,  0.0131, -0.0296,\n",
      "          0.0194, -0.0848, -0.0616,  0.0829,  0.0771, -0.0916,  0.0054, -0.0674,\n",
      "          0.0425, -0.1077,  0.0424,  0.1077, -0.0918, -0.0262, -0.0878,  0.0493,\n",
      "          0.0453, -0.0220, -0.0382,  0.1060,  0.0417,  0.0691, -0.0597, -0.1089,\n",
      "         -0.0699,  0.0176,  0.1046, -0.0606,  0.0611, -0.0714, -0.0717,  0.0107,\n",
      "         -0.0341, -0.0691, -0.0660,  0.0356, -0.0633,  0.0108,  0.1000, -0.0587,\n",
      "          0.0478, -0.0462,  0.0531, -0.0375,  0.0923, -0.0717,  0.0287,  0.0024,\n",
      "          0.1008, -0.0387, -0.0262,  0.0965, -0.0998,  0.0203,  0.0851, -0.0458,\n",
      "          0.0474, -0.0325, -0.0032,  0.1062,  0.0247, -0.0193, -0.0165, -0.0603,\n",
      "         -0.0591,  0.0978, -0.0762, -0.1012, -0.0153,  0.0345, -0.0581, -0.0572,\n",
      "         -0.0736, -0.0121, -0.0456,  0.0914],\n",
      "        [-0.1043,  0.0456, -0.1063,  0.0966,  0.0597, -0.0799,  0.1080, -0.0520,\n",
      "         -0.0663,  0.0396,  0.0344, -0.0917, -0.0258,  0.0699,  0.0887, -0.0587,\n",
      "          0.0445,  0.0692, -0.0692,  0.0568,  0.0651, -0.0543,  0.0135, -0.0984,\n",
      "          0.0191, -0.1043, -0.0893, -0.0542, -0.0542, -0.0906, -0.0906,  0.0441,\n",
      "          0.0047, -0.1059, -0.0259,  0.0298, -0.0836, -0.0166, -0.0142, -0.1011,\n",
      "         -0.1079, -0.0572,  0.0204,  0.0427, -0.0979,  0.0028,  0.0950, -0.0371,\n",
      "         -0.0790, -0.0096,  0.0984,  0.0211, -0.0714, -0.0508, -0.0858,  0.0272,\n",
      "         -0.0012, -0.0195,  0.0041, -0.0739, -0.0166,  0.0962,  0.0724,  0.0080,\n",
      "          0.0124, -0.0102,  0.0921, -0.0365,  0.0133,  0.0621,  0.0971, -0.0015,\n",
      "         -0.0695, -0.0620, -0.1081,  0.0392,  0.0982, -0.1085,  0.1027,  0.0766,\n",
      "          0.0994, -0.0690,  0.0629, -0.0442],\n",
      "        [ 0.0337,  0.0854, -0.0564, -0.1017, -0.1048, -0.0527,  0.0633,  0.0354,\n",
      "          0.0196, -0.0090, -0.0535,  0.0917,  0.0928, -0.0456,  0.0616, -0.0069,\n",
      "          0.0988, -0.0374, -0.0634,  0.0154, -0.0418, -0.1054,  0.0535, -0.0219,\n",
      "         -0.0027, -0.0180, -0.0589, -0.0465, -0.0869,  0.0147, -0.0675, -0.0184,\n",
      "         -0.0693,  0.0529, -0.0690, -0.1005,  0.1013,  0.0945,  0.0474, -0.0443,\n",
      "         -0.0125, -0.0111,  0.0646,  0.0356, -0.1046,  0.0869, -0.0516, -0.0184,\n",
      "         -0.0584, -0.0940, -0.0506, -0.0610, -0.0549,  0.0914,  0.0387,  0.1038,\n",
      "         -0.0554,  0.0245,  0.1034, -0.0643,  0.1079, -0.0685, -0.0247,  0.0058,\n",
      "         -0.0727,  0.0993,  0.1069,  0.0732, -0.0089,  0.0040, -0.0004, -0.0200,\n",
      "         -0.0120, -0.0868, -0.0309,  0.0246, -0.1044, -0.0843,  0.0025, -0.0741,\n",
      "          0.0893, -0.0011, -0.0945,  0.0256],\n",
      "        [ 0.0024,  0.0917, -0.0604,  0.0154, -0.0929,  0.0895, -0.0426, -0.0285,\n",
      "         -0.0979,  0.0822,  0.0225, -0.0478, -0.0648,  0.1046,  0.0608, -0.0851,\n",
      "         -0.0320, -0.0981, -0.0085,  0.0589,  0.0628,  0.1063, -0.0393, -0.0066,\n",
      "          0.0677,  0.0784, -0.0860, -0.0567,  0.0754,  0.0681,  0.0143,  0.0804,\n",
      "          0.0252,  0.1080, -0.0657, -0.0929,  0.0064, -0.0286,  0.0173, -0.0086,\n",
      "         -0.0691, -0.0070, -0.0975, -0.0359,  0.0086,  0.0512, -0.0822,  0.0151,\n",
      "         -0.0574,  0.0053, -0.0136,  0.0049, -0.0693,  0.0016, -0.0655, -0.0492,\n",
      "         -0.0458, -0.0713, -0.1072, -0.0497, -0.0391, -0.0619,  0.0244, -0.0475,\n",
      "         -0.1077, -0.0936,  0.0625, -0.0506,  0.0590,  0.0560, -0.0966,  0.0521,\n",
      "          0.0455,  0.0216,  0.0028,  0.1000, -0.0824,  0.0461,  0.1029,  0.1088,\n",
      "         -0.0913, -0.0091,  0.0333,  0.0192],\n",
      "        [-0.0905,  0.0450,  0.0203,  0.0215,  0.0483, -0.0225, -0.0321, -0.0319,\n",
      "         -0.0339, -0.0077,  0.0916, -0.0875,  0.0271,  0.0925,  0.0419,  0.0706,\n",
      "          0.0584, -0.1069, -0.0438, -0.0265, -0.0260, -0.1056,  0.0096, -0.0246,\n",
      "          0.0669, -0.0824,  0.1009,  0.0800,  0.0581,  0.0923,  0.0171,  0.0980,\n",
      "          0.0401, -0.0871,  0.0262, -0.0795,  0.0819,  0.0207,  0.0486,  0.1021,\n",
      "          0.0963,  0.0823, -0.0379, -0.0661,  0.0943, -0.0179,  0.0372,  0.0293,\n",
      "         -0.0168,  0.0341, -0.0162,  0.0283, -0.0417,  0.0531,  0.0181, -0.0887,\n",
      "          0.0820, -0.0452,  0.0081,  0.0446, -0.1055,  0.0669, -0.0996,  0.0753,\n",
      "         -0.0781,  0.0459, -0.0013,  0.0400, -0.0888,  0.0309,  0.0245,  0.0091,\n",
      "          0.0392,  0.0671, -0.0504, -0.0021, -0.0126, -0.0783,  0.0816, -0.0313,\n",
      "          0.0235,  0.0218,  0.0145, -0.0386],\n",
      "        [ 0.0884, -0.0319, -0.0104,  0.0440,  0.0686, -0.0916, -0.0430,  0.1017,\n",
      "         -0.0973, -0.0919, -0.0093, -0.0211,  0.0500, -0.0986,  0.0459,  0.0070,\n",
      "         -0.0171, -0.0339, -0.0922, -0.0145,  0.0535,  0.0867, -0.1080,  0.0762,\n",
      "          0.0986, -0.0470, -0.0806,  0.1013, -0.0166,  0.0811,  0.0820, -0.0624,\n",
      "         -0.0495, -0.0828, -0.0968,  0.0597,  0.0935,  0.0538, -0.0298, -0.1070,\n",
      "         -0.1076,  0.0674, -0.0925, -0.0538, -0.0170,  0.0671,  0.0095,  0.0437,\n",
      "         -0.0104,  0.0175,  0.0266,  0.0076, -0.0449,  0.0706, -0.0465, -0.0539,\n",
      "         -0.0690, -0.0318,  0.0962, -0.0696,  0.0401,  0.0572, -0.0680, -0.0400,\n",
      "          0.0882, -0.0249, -0.0639,  0.0148,  0.0818,  0.0129,  0.0765, -0.0592,\n",
      "          0.0085, -0.0319, -0.0715, -0.0730,  0.0540,  0.0039, -0.0960,  0.0145,\n",
      "          0.0494,  0.0507,  0.0434, -0.0090],\n",
      "        [ 0.0128, -0.0738,  0.0424, -0.0894, -0.0240, -0.0534,  0.0988,  0.0998,\n",
      "          0.0095,  0.0286,  0.0410,  0.1044, -0.0848, -0.0649,  0.0426,  0.0316,\n",
      "          0.0628,  0.0976, -0.1080, -0.0682, -0.0246, -0.0616,  0.0267, -0.0145,\n",
      "         -0.0242,  0.0397,  0.0277, -0.0035,  0.1058, -0.0944,  0.0740, -0.0697,\n",
      "          0.0065,  0.0241,  0.0199, -0.0579, -0.0944, -0.0920, -0.0118, -0.0241,\n",
      "         -0.0357,  0.0645, -0.0975, -0.0646,  0.0531, -0.0056,  0.0219, -0.0028,\n",
      "         -0.0113, -0.0307,  0.0090, -0.0280, -0.0295,  0.0178, -0.0211, -0.0427,\n",
      "          0.0445, -0.0448,  0.0764, -0.0035,  0.0728, -0.1060, -0.0541, -0.0182,\n",
      "         -0.0473,  0.0514, -0.0666, -0.0391, -0.0650,  0.0576,  0.0161, -0.0339,\n",
      "         -0.0788, -0.0935, -0.0412,  0.0937, -0.0093,  0.0522,  0.0817, -0.0537,\n",
      "          0.0221,  0.0764, -0.0268, -0.1028]])), ('fc3.bias', tensor([-0.0313,  0.0209,  0.0476,  0.0683, -0.0977, -0.0495, -0.0577, -0.0394,\n",
      "        -0.0898, -0.0937]))])\n",
      "OrderedDict([('fc1.weight', tensor([[-0.0160, -0.0212, -0.0273,  ...,  0.0130,  0.0182, -0.0287],\n",
      "        [ 0.0326, -0.0278, -0.0140,  ..., -0.0042, -0.0075,  0.0074],\n",
      "        [-0.0337, -0.0229, -0.0118,  ..., -0.0055,  0.0071, -0.0204],\n",
      "        ...,\n",
      "        [-0.0110, -0.0346, -0.0273,  ..., -0.0124, -0.0041, -0.0257],\n",
      "        [-0.0155,  0.0148, -0.0201,  ..., -0.0308,  0.0139, -0.0199],\n",
      "        [-0.0125, -0.0209,  0.0144,  ..., -0.0100, -0.0181,  0.0191]])), ('fc1.bias', tensor([-8.1783e-03, -4.0504e-03, -4.5305e-03,  3.7464e-03, -2.3053e-02,\n",
      "        -1.1192e-02,  1.3359e-02,  1.3025e-02,  2.5128e-03, -1.5918e-02,\n",
      "        -2.1895e-02, -3.2113e-02,  1.4816e-02, -1.0729e-02, -2.5456e-02,\n",
      "         2.5900e-03, -2.2065e-02,  1.2159e-02, -1.4581e-02, -2.8635e-02,\n",
      "         2.6172e-02,  1.4813e-02, -1.6591e-02,  2.8914e-02,  8.3991e-03,\n",
      "         3.5289e-02,  2.0257e-02, -2.9931e-03, -3.4280e-02, -3.3169e-02,\n",
      "         3.3885e-03, -7.0055e-03,  3.8402e-03,  1.5720e-02,  2.8990e-02,\n",
      "         2.6398e-02, -2.0791e-02, -2.7742e-02, -3.2212e-02,  2.0655e-02,\n",
      "        -2.5958e-02,  1.5325e-03, -3.3196e-02, -1.8939e-02, -9.7937e-03,\n",
      "         2.0505e-02, -8.9423e-03, -3.2709e-02,  5.4826e-03,  3.0599e-02,\n",
      "         2.9819e-02, -2.9774e-02,  3.4083e-02, -1.7792e-04,  5.0857e-03,\n",
      "         3.1751e-02,  1.3319e-02,  2.4161e-02, -1.3450e-03,  1.2160e-02,\n",
      "         2.0664e-02,  9.0005e-03, -2.5300e-02, -1.1255e-02,  2.6485e-02,\n",
      "        -1.1517e-02,  1.8078e-04,  2.7818e-02, -7.4897e-03, -2.6586e-02,\n",
      "        -2.6365e-02, -1.5715e-02, -1.5366e-03, -3.4232e-02,  1.2743e-03,\n",
      "        -1.0752e-02,  2.4306e-02, -7.3850e-03,  2.1861e-02, -1.2935e-02,\n",
      "        -1.6325e-02, -1.0905e-02, -1.4125e-03, -6.9753e-03,  9.8532e-03,\n",
      "        -2.3739e-02, -1.3287e-02,  2.1444e-02,  1.0012e-02, -7.9820e-03,\n",
      "        -1.5324e-02,  2.0259e-02, -1.3531e-04,  3.5574e-02, -3.0214e-02,\n",
      "        -3.1566e-02, -3.1190e-02, -1.1490e-02, -3.0982e-02,  2.1199e-03,\n",
      "        -4.0552e-03,  4.9557e-03, -2.6082e-02,  5.3518e-03,  2.3230e-02,\n",
      "        -2.8697e-02,  3.1880e-02,  3.5564e-02, -3.3192e-02,  3.3738e-02,\n",
      "         8.7671e-03,  5.3287e-03,  2.7316e-02, -2.4400e-02,  1.5509e-02,\n",
      "         2.4702e-02,  2.2126e-02,  2.9866e-02, -1.2776e-02,  4.9522e-03,\n",
      "        -2.9150e-02, -2.3585e-02, -3.3362e-02, -2.7223e-02, -2.1659e-02,\n",
      "        -3.3897e-02,  2.2876e-02, -1.5457e-02,  8.8740e-03, -9.3096e-03,\n",
      "         1.0760e-02,  2.8107e-02,  1.1868e-02, -2.5262e-02, -8.6447e-03,\n",
      "         1.0109e-02, -6.7530e-03, -2.1921e-02, -3.5678e-02, -8.4031e-03,\n",
      "        -3.1896e-03,  2.8859e-02, -4.5577e-03,  5.3769e-03, -2.9069e-02,\n",
      "         1.8127e-02, -2.4214e-02,  2.9473e-02,  1.1907e-02, -1.9439e-02,\n",
      "        -3.1723e-02,  1.8987e-02,  2.5519e-02,  3.3821e-02, -1.8920e-02,\n",
      "         2.9933e-02, -3.2514e-02,  3.4374e-02, -3.9277e-03, -2.4720e-02,\n",
      "        -3.2739e-02,  2.1121e-02, -2.3581e-03, -1.3191e-02,  7.3649e-03,\n",
      "        -5.2457e-03, -1.8779e-02, -1.7212e-02,  9.2062e-03, -1.8229e-02,\n",
      "         2.5850e-02, -2.0338e-02,  2.5556e-02, -3.1176e-02, -3.3060e-02,\n",
      "        -2.3649e-02,  2.2334e-02,  1.6940e-02,  2.6461e-02, -2.5518e-02,\n",
      "         1.2793e-02, -2.6655e-02,  2.4554e-02,  4.7252e-03,  2.0676e-02,\n",
      "         2.3619e-02,  8.1710e-03,  3.3620e-02,  1.6598e-02,  4.7244e-03,\n",
      "        -2.4267e-02, -3.3942e-03, -2.6219e-02, -8.7251e-03,  1.3661e-03,\n",
      "        -9.9371e-03, -1.6310e-02,  2.2092e-02, -2.3138e-03, -6.9472e-03,\n",
      "         6.5081e-06,  6.4449e-03, -1.7217e-02,  1.6033e-02, -3.2068e-02,\n",
      "         6.6322e-03,  7.9852e-04, -2.5940e-02,  2.8878e-02, -3.3202e-02,\n",
      "        -2.5618e-03,  1.3792e-02, -6.7875e-03,  8.9559e-03, -2.1766e-02,\n",
      "         2.7338e-02, -3.1875e-02,  3.3945e-02, -9.1741e-03,  1.5823e-02,\n",
      "         9.3066e-03, -3.2856e-03,  3.0995e-02,  1.0511e-02, -1.8979e-02,\n",
      "        -2.7732e-02,  9.7300e-03, -1.2734e-02, -2.1122e-02, -1.9314e-02,\n",
      "         1.3393e-02,  1.7943e-02, -3.0585e-02, -1.9257e-02, -3.0634e-02,\n",
      "        -1.8237e-02, -2.5004e-02,  2.5601e-02,  2.4433e-02, -2.9705e-02,\n",
      "         3.3498e-02,  3.2611e-02,  5.8611e-03,  2.1598e-02, -2.3410e-02,\n",
      "         1.3916e-02, -2.6831e-02, -1.5834e-02,  2.2291e-02, -2.3766e-02,\n",
      "        -9.8343e-03,  9.9164e-03,  2.1428e-02, -1.8078e-02,  2.1534e-02,\n",
      "        -3.2088e-02])), ('fc2.weight', tensor([[ 0.0029,  0.0404, -0.0395,  ..., -0.0524, -0.0597, -0.0231],\n",
      "        [ 0.0228,  0.0590, -0.0261,  ..., -0.0455, -0.0608, -0.0430],\n",
      "        [-0.0354, -0.0194,  0.0245,  ...,  0.0010, -0.0227,  0.0301],\n",
      "        ...,\n",
      "        [-0.0590,  0.0182, -0.0522,  ...,  0.0501,  0.0072,  0.0436],\n",
      "        [-0.0422, -0.0483,  0.0182,  ..., -0.0048,  0.0322, -0.0398],\n",
      "        [-0.0601, -0.0014,  0.0434,  ..., -0.0070,  0.0202, -0.0280]])), ('fc2.bias', tensor([ 0.0226, -0.0567,  0.0055, -0.0153,  0.0529,  0.0554, -0.0506,  0.0120,\n",
      "         0.0183,  0.0219, -0.0443, -0.0423, -0.0046,  0.0372,  0.0019,  0.0476,\n",
      "        -0.0008, -0.0490,  0.0385,  0.0007, -0.0193, -0.0268,  0.0089,  0.0093,\n",
      "        -0.0246,  0.0523, -0.0547,  0.0547, -0.0516, -0.0568, -0.0343, -0.0618,\n",
      "        -0.0599,  0.0207,  0.0067, -0.0228, -0.0116, -0.0500, -0.0598, -0.0506,\n",
      "        -0.0246, -0.0276,  0.0002, -0.0423, -0.0176,  0.0573, -0.0354, -0.0198,\n",
      "         0.0588,  0.0491,  0.0241, -0.0306,  0.0024, -0.0236,  0.0496, -0.0410,\n",
      "         0.0260,  0.0227,  0.0188,  0.0088,  0.0090, -0.0139,  0.0598,  0.0302,\n",
      "         0.0111,  0.0330, -0.0148,  0.0332, -0.0246,  0.0149,  0.0283,  0.0186,\n",
      "         0.0613, -0.0494,  0.0044,  0.0458,  0.0526,  0.0400,  0.0593, -0.0470,\n",
      "         0.0395, -0.0434,  0.0071,  0.0514,  0.0416, -0.0606,  0.0171,  0.0563,\n",
      "         0.0064, -0.0387,  0.0238, -0.0567, -0.0436, -0.0378,  0.0316,  0.0118,\n",
      "        -0.0023, -0.0506,  0.0145, -0.0212,  0.0266,  0.0149,  0.0476, -0.0154,\n",
      "        -0.0090,  0.0151, -0.0403,  0.0252,  0.0327, -0.0067, -0.0383, -0.0605,\n",
      "         0.0295, -0.0515, -0.0509,  0.0173,  0.0462, -0.0578, -0.0279,  0.0606])), ('fc3.weight', tensor([[-0.0910,  0.0886,  0.0763,  ..., -0.0906, -0.0373, -0.0437],\n",
      "        [ 0.0615, -0.0245, -0.0878,  ..., -0.0863,  0.0912,  0.0584],\n",
      "        [ 0.0142,  0.0487, -0.0112,  ...,  0.0723,  0.0389,  0.0325],\n",
      "        ...,\n",
      "        [-0.0325,  0.0160, -0.0217,  ...,  0.0236, -0.0124, -0.0557],\n",
      "        [-0.0756,  0.0003, -0.0593,  ..., -0.0623, -0.0736, -0.0187],\n",
      "        [ 0.0634,  0.0551, -0.0331,  ...,  0.0530, -0.0745, -0.0515]])), ('fc3.bias', tensor([-0.0796, -0.0640,  0.0605, -0.0172,  0.0220, -0.0774,  0.0652,  0.0015,\n",
      "        -0.0584, -0.0449, -0.0573, -0.0295,  0.0817, -0.0479,  0.0787,  0.0268,\n",
      "        -0.0539,  0.0799,  0.0860,  0.0562, -0.0697, -0.0271, -0.0397,  0.0752,\n",
      "        -0.0561, -0.0641, -0.0480,  0.0616, -0.0496, -0.0153, -0.0329, -0.0506,\n",
      "        -0.0220,  0.0013,  0.0596, -0.0560, -0.0346,  0.0776,  0.0008, -0.0641,\n",
      "         0.0379,  0.0220, -0.0177,  0.0508, -0.0838, -0.0619,  0.0827,  0.0044,\n",
      "         0.0481,  0.0573,  0.0485,  0.0125, -0.0642,  0.0270,  0.0509, -0.0488,\n",
      "         0.0736,  0.0519, -0.0181, -0.0024, -0.0501, -0.0702, -0.0637, -0.0296,\n",
      "         0.0428, -0.0216,  0.0198, -0.0629,  0.0404, -0.0776, -0.0229,  0.0242,\n",
      "         0.0031,  0.0694, -0.0115, -0.0242, -0.0227,  0.0158, -0.0893,  0.0528,\n",
      "         0.0031,  0.0389,  0.0253,  0.0203])), ('fc4.weight', tensor([[ 9.5816e-02,  1.9830e-02,  5.0565e-02,  1.0794e-01,  7.6175e-02,\n",
      "          9.3346e-02,  5.9202e-02,  7.5396e-02,  7.7664e-02,  8.0304e-02,\n",
      "          8.4320e-02,  5.8568e-02,  9.9667e-02,  6.5423e-02,  3.7427e-03,\n",
      "         -8.4483e-02,  7.7184e-02, -8.3241e-02, -4.1983e-02, -7.6093e-02,\n",
      "          9.8312e-02, -7.3686e-03, -2.7097e-02,  8.2802e-02,  9.1847e-02,\n",
      "          4.4622e-04, -2.8288e-02, -2.6278e-02,  6.1297e-02, -6.4639e-02,\n",
      "          9.1377e-02, -2.5425e-02, -8.2318e-02, -5.0539e-02,  1.0781e-01,\n",
      "         -3.6292e-02, -5.0541e-02,  5.7756e-02, -6.3230e-02,  8.0832e-02,\n",
      "          9.2883e-02, -1.0755e-01,  2.1082e-02, -7.5090e-02,  1.0078e-01,\n",
      "          1.1250e-02, -3.2079e-02, -7.6037e-02, -4.2494e-02, -5.1894e-02,\n",
      "          6.2312e-02,  2.8596e-02,  5.2048e-02,  4.4062e-02, -4.0400e-02,\n",
      "          3.0048e-02,  1.0673e-01,  1.0170e-01, -4.9909e-02,  2.6588e-02,\n",
      "         -1.0690e-04,  2.5496e-02,  8.1610e-04,  1.7971e-02, -7.7458e-03,\n",
      "         -5.3920e-02, -6.7563e-02,  9.1807e-02, -1.0661e-01,  2.1694e-02,\n",
      "         -5.5907e-02, -6.7608e-02,  6.4765e-02,  4.5812e-02, -1.0881e-01,\n",
      "         -7.4577e-02,  4.7000e-02, -1.0015e-01, -5.2752e-02, -1.0409e-01,\n",
      "         -3.1028e-02,  4.8292e-02, -2.0326e-02, -4.3736e-02],\n",
      "        [-3.8865e-02,  1.0829e-01, -9.7216e-02, -7.0066e-02, -2.9516e-02,\n",
      "          3.1708e-02,  8.9315e-02,  8.8948e-03,  1.0535e-01,  6.9634e-02,\n",
      "          1.0230e-01, -3.4485e-02,  7.2996e-02,  4.3739e-03, -7.2283e-03,\n",
      "          4.4048e-02,  8.9141e-02,  2.7990e-02,  8.3076e-02, -4.9744e-02,\n",
      "         -2.9761e-02,  6.2206e-02, -1.6574e-02, -5.5964e-02,  6.3645e-02,\n",
      "          7.1648e-02,  7.6276e-02, -6.6960e-02,  9.2054e-02, -1.0444e-01,\n",
      "         -3.2484e-02,  1.1240e-02, -7.4411e-02, -4.8946e-02, -9.5811e-02,\n",
      "         -8.2742e-02, -5.5136e-02, -1.0387e-01,  4.4550e-02,  5.3822e-02,\n",
      "         -2.4275e-02,  4.3302e-02,  1.0851e-01,  3.4159e-03, -4.7636e-02,\n",
      "         -5.1779e-02,  2.5925e-02,  1.0240e-01,  5.3728e-03, -7.1704e-02,\n",
      "         -9.5515e-02,  6.6375e-02, -8.8239e-02, -3.0309e-02,  7.7862e-02,\n",
      "         -6.7908e-02, -1.0084e-01,  1.3614e-02, -3.2257e-02,  1.0422e-01,\n",
      "          9.4734e-02, -8.1674e-02,  2.6103e-03,  2.2620e-02, -2.9037e-02,\n",
      "          8.3524e-02,  1.1426e-02,  8.4429e-02, -4.6069e-02, -2.5969e-02,\n",
      "         -3.1030e-02,  7.4086e-02, -1.0427e-01,  2.4201e-02, -5.4639e-02,\n",
      "         -6.8720e-02, -1.0513e-01,  1.9038e-02, -7.6640e-03, -4.4188e-02,\n",
      "          4.4779e-02,  6.0316e-03,  4.0905e-02, -7.5704e-02],\n",
      "        [ 5.4922e-02, -3.9226e-02,  7.7882e-02,  4.6926e-02, -8.3735e-04,\n",
      "          8.1957e-02, -3.1770e-02,  5.7522e-02, -2.5117e-02,  1.6938e-02,\n",
      "         -2.9945e-02, -5.1975e-02, -4.7699e-02, -3.4478e-03, -4.3610e-02,\n",
      "          8.9194e-02,  1.6009e-02,  8.8014e-02,  6.4292e-02, -8.3065e-02,\n",
      "         -3.4290e-02, -9.7074e-03,  8.1068e-02, -4.1370e-02, -8.8831e-02,\n",
      "         -2.7878e-02, -6.0224e-02, -8.3136e-02, -3.1715e-02, -8.0538e-02,\n",
      "         -7.5076e-02,  3.9383e-02,  1.2282e-02,  2.5878e-02,  1.0037e-01,\n",
      "         -1.0732e-01,  3.1075e-02,  7.3910e-02, -1.6919e-02, -9.9469e-02,\n",
      "          3.6007e-02,  9.9196e-02,  1.8885e-03,  4.1475e-02, -1.0355e-01,\n",
      "         -8.6055e-02, -4.5138e-04, -6.6807e-02,  8.4797e-02,  6.7396e-02,\n",
      "          5.8827e-02,  3.7801e-02, -8.8962e-03,  9.6218e-02,  4.8666e-02,\n",
      "         -9.4346e-02,  2.2716e-02, -9.2209e-02, -5.5227e-02, -3.5008e-02,\n",
      "          4.0948e-02,  8.0154e-02,  8.9938e-02,  4.3498e-02,  1.0563e-01,\n",
      "         -7.1969e-02,  6.4185e-03, -7.3374e-02,  3.3474e-02, -1.1439e-02,\n",
      "          1.5435e-02, -4.8718e-02,  4.0316e-02, -3.4504e-02,  7.9154e-02,\n",
      "         -4.6856e-02, -9.2404e-02, -3.7428e-02,  6.8647e-03,  2.1289e-03,\n",
      "         -1.4161e-02,  4.5024e-02, -8.2936e-02, -6.5395e-02],\n",
      "        [ 3.2044e-02,  6.2299e-02, -4.9655e-02,  8.0131e-03,  7.8644e-03,\n",
      "         -1.0409e-03,  7.3863e-02, -6.4735e-02, -1.0928e-02,  1.8279e-02,\n",
      "          2.8344e-02, -1.0852e-02, -9.6656e-02,  9.8391e-02, -1.0193e-02,\n",
      "         -5.9126e-02, -2.5602e-02, -5.5922e-02,  8.7200e-03,  4.3171e-02,\n",
      "         -8.4966e-02,  7.8728e-02, -2.1425e-02, -2.7807e-02, -4.0787e-02,\n",
      "         -7.8077e-02, -9.0284e-03,  2.3541e-02, -1.2389e-02,  1.0757e-03,\n",
      "         -2.2148e-03,  3.0600e-02, -1.9606e-03, -9.2187e-02, -4.3174e-02,\n",
      "          9.6637e-02,  3.2999e-02,  9.4988e-02,  5.3993e-03, -6.4246e-02,\n",
      "         -2.6292e-02, -5.7650e-02,  2.7080e-02,  6.1891e-02, -1.0161e-01,\n",
      "          7.9586e-02,  7.4837e-02,  5.9895e-02,  5.9665e-02, -1.0633e-01,\n",
      "         -8.7443e-02,  1.7374e-02, -9.5276e-02, -3.7025e-02,  2.5978e-02,\n",
      "         -6.0332e-02,  1.0286e-01, -9.8456e-02,  7.9432e-02,  7.1703e-02,\n",
      "          8.4327e-02, -5.9901e-02, -8.1992e-02,  1.8302e-02, -7.0597e-02,\n",
      "          3.9267e-02,  3.7101e-02, -8.8613e-02,  7.5139e-03, -2.2806e-02,\n",
      "         -7.2592e-02,  3.4226e-02, -4.9076e-02,  3.6783e-02,  6.5491e-02,\n",
      "         -8.4212e-02,  8.9686e-02, -6.0740e-02,  8.2122e-02,  6.3430e-02,\n",
      "         -7.5394e-02,  4.2968e-02, -1.2671e-02, -4.4415e-02],\n",
      "        [ 9.2597e-02, -5.8699e-02, -9.8929e-02,  5.1799e-02,  9.2723e-02,\n",
      "          6.2631e-02, -6.8971e-03,  3.7223e-02,  5.1057e-02,  6.6578e-02,\n",
      "         -5.3956e-02,  1.0164e-01, -4.9023e-02,  5.5288e-02, -6.2142e-02,\n",
      "          6.2904e-02, -5.7035e-02, -2.2633e-02, -3.0055e-03, -3.9041e-02,\n",
      "         -8.6268e-02, -4.6936e-02, -2.1205e-02,  1.4567e-02,  8.5801e-02,\n",
      "          3.0069e-02, -8.8757e-02,  9.6388e-02, -6.2133e-02, -3.2856e-02,\n",
      "         -1.0280e-01,  3.8362e-02,  5.6092e-02,  3.7213e-02,  5.5387e-02,\n",
      "         -6.9487e-02,  8.3268e-02,  1.6889e-03, -7.8919e-02, -4.4862e-03,\n",
      "         -1.8932e-02,  8.0872e-02,  2.2400e-02, -9.3826e-02,  2.9240e-02,\n",
      "         -4.6365e-02,  3.1018e-02,  1.0331e-01,  5.0716e-03,  9.0450e-02,\n",
      "          7.7248e-02,  3.8780e-03,  5.2752e-02, -6.7595e-02,  6.8199e-02,\n",
      "          2.9222e-02,  2.2572e-02, -8.6792e-02,  8.0348e-02, -4.0125e-03,\n",
      "         -1.0228e-01,  6.7796e-02, -2.5739e-03, -6.8093e-03, -1.0434e-01,\n",
      "         -2.8249e-02,  1.0077e-01,  1.0121e-01,  2.5433e-02, -7.2995e-02,\n",
      "          9.6693e-02, -6.4735e-02, -8.0044e-02, -6.6666e-02, -4.7916e-02,\n",
      "          1.0801e-01, -3.6181e-02, -2.6089e-02,  7.5530e-02,  9.2217e-02,\n",
      "          7.0642e-02, -1.0018e-01, -3.3311e-02, -1.0613e-01],\n",
      "        [ 2.1478e-02, -8.2239e-02,  5.4983e-02,  4.7067e-02, -9.8418e-02,\n",
      "         -1.8815e-02,  1.0488e-01, -8.7400e-02, -7.0472e-02, -5.0802e-02,\n",
      "          5.7257e-02,  2.3368e-02,  7.3130e-02,  8.2352e-03, -2.3110e-02,\n",
      "         -6.5240e-02,  7.6638e-02, -1.0307e-01, -2.4227e-02,  9.5315e-02,\n",
      "          3.1691e-02,  5.7814e-02,  3.8552e-02, -6.1244e-02,  3.3057e-02,\n",
      "          3.6737e-02,  6.3744e-03,  3.8537e-02,  9.4569e-02, -5.0501e-02,\n",
      "         -6.9549e-03, -8.4603e-02,  9.2814e-02,  7.8038e-02,  1.0443e-01,\n",
      "          4.7804e-02, -5.0396e-02, -2.2561e-02,  4.7819e-02, -6.2499e-02,\n",
      "         -1.7505e-02, -5.5850e-02, -6.7080e-02, -4.9475e-02,  3.2538e-02,\n",
      "         -5.9395e-02, -6.5525e-02, -9.5156e-02, -2.7980e-02,  8.4277e-02,\n",
      "          8.9962e-02,  4.2744e-02,  7.4886e-02, -4.5615e-02, -9.1628e-02,\n",
      "         -4.1853e-02, -8.9890e-02,  5.5412e-02,  5.1382e-03,  4.0183e-02,\n",
      "         -4.6145e-03, -1.0880e-01,  5.2031e-02,  1.1244e-02, -8.5243e-02,\n",
      "          4.1369e-02, -1.6647e-02, -8.3803e-02, -2.1387e-02, -7.9145e-02,\n",
      "         -2.7728e-02, -5.3359e-03, -6.1553e-02,  8.2116e-02, -7.3805e-04,\n",
      "          1.9616e-03,  8.5500e-03, -9.2453e-03, -6.8748e-02,  5.1803e-02,\n",
      "          1.0322e-01, -3.1202e-02, -7.7237e-02,  6.2467e-02],\n",
      "        [-5.2034e-02,  3.1900e-02,  2.3372e-02, -2.9282e-04, -1.0218e-01,\n",
      "         -2.8422e-02, -3.6758e-02, -6.4510e-02, -1.4727e-02, -8.0974e-02,\n",
      "          2.6853e-02,  2.6763e-02,  6.9469e-02, -5.3894e-02, -7.6431e-02,\n",
      "         -1.9391e-02, -5.0916e-02,  5.8805e-02, -5.1504e-02,  8.4690e-03,\n",
      "         -5.3260e-02, -5.1105e-02, -3.5361e-02,  9.3809e-02, -5.0023e-02,\n",
      "         -7.2369e-02,  4.9545e-02,  7.8323e-02,  1.6801e-02,  8.9409e-02,\n",
      "         -3.7778e-02,  8.4723e-02,  2.0028e-02, -2.6740e-02,  1.1347e-04,\n",
      "          9.3987e-02,  5.5145e-02, -4.1381e-03,  8.8289e-02, -5.0908e-02,\n",
      "         -7.9954e-02,  3.8242e-02,  1.1059e-02,  1.7414e-02,  6.4591e-02,\n",
      "         -3.0869e-02,  3.3846e-02, -5.0548e-02, -4.3951e-02, -5.2688e-02,\n",
      "          9.6921e-02,  5.5852e-02, -5.5089e-02, -2.4114e-02,  6.4986e-02,\n",
      "          3.7760e-02,  2.8518e-03,  4.6775e-02, -4.0206e-02, -8.3712e-02,\n",
      "          1.8960e-02, -8.0848e-02, -7.8056e-03,  7.8413e-03, -2.4248e-02,\n",
      "          3.8781e-02,  4.7126e-02,  1.0498e-01, -2.3860e-03, -1.9049e-02,\n",
      "          8.0767e-02, -8.2945e-02,  3.2691e-02,  5.9385e-02,  5.4235e-02,\n",
      "         -6.4151e-02,  3.9736e-02,  4.0750e-03,  9.4675e-02,  9.1592e-02,\n",
      "          9.2488e-02,  6.5385e-02, -1.1573e-02, -1.2964e-02],\n",
      "        [ 6.2948e-02,  2.5669e-02, -6.2447e-02,  4.9145e-02,  9.4823e-03,\n",
      "         -3.4296e-02,  6.2473e-02,  1.2896e-02, -5.1663e-02,  6.5933e-02,\n",
      "         -9.4920e-02,  1.1910e-02,  7.0601e-02,  8.7929e-02,  1.1195e-02,\n",
      "         -5.7021e-02,  4.2342e-02, -2.9079e-02,  1.0504e-01,  6.4722e-02,\n",
      "         -7.9303e-02,  3.3081e-02, -1.0449e-01,  7.9367e-02, -1.9130e-02,\n",
      "          1.5248e-02, -9.5407e-02, -3.3337e-02,  1.3185e-02, -5.8275e-02,\n",
      "         -3.0169e-02,  2.2312e-02,  8.9283e-02, -5.8951e-02, -6.6255e-02,\n",
      "          5.5310e-02,  1.0876e-01, -3.0343e-02,  8.0449e-02, -8.2584e-02,\n",
      "         -9.8157e-02,  8.6473e-02,  3.1518e-02,  1.0699e-01, -4.2625e-02,\n",
      "         -9.7696e-02, -1.0374e-01, -1.0492e-01,  6.2050e-02, -8.6655e-02,\n",
      "          6.7589e-02,  3.9299e-02, -1.0551e-01,  8.5029e-02,  1.0310e-01,\n",
      "         -1.0091e-01, -2.7921e-03,  4.3825e-02,  9.7360e-02, -4.8845e-02,\n",
      "         -6.8016e-02,  1.1310e-02,  1.0110e-01, -5.4881e-02,  5.7246e-02,\n",
      "         -4.4783e-02,  4.4411e-02,  7.0399e-02, -8.3257e-02, -9.7396e-02,\n",
      "          7.5762e-02,  1.5029e-02,  7.4447e-02,  8.3165e-02,  4.4052e-02,\n",
      "         -6.2214e-02,  8.2866e-03,  4.9017e-02, -5.8703e-02,  1.0268e-01,\n",
      "         -2.1267e-03,  3.1928e-02,  4.7113e-02, -1.0808e-01],\n",
      "        [ 4.9989e-02, -2.9703e-02, -1.1785e-02,  5.5032e-02,  7.5100e-02,\n",
      "          1.0711e-01, -4.2680e-02, -5.9141e-02,  7.9934e-02, -4.9075e-02,\n",
      "         -3.8547e-02,  5.4072e-02, -5.0194e-03, -1.7208e-02, -3.2233e-03,\n",
      "         -7.7003e-02,  2.8676e-02, -1.0695e-01, -7.6614e-03,  1.0073e-02,\n",
      "          1.0280e-01,  1.7207e-02, -1.0010e-01,  9.4073e-02, -7.1506e-03,\n",
      "          6.8113e-02,  1.0533e-01,  8.1043e-02,  1.3904e-02, -9.4600e-02,\n",
      "          8.7969e-02, -8.0203e-02,  3.2143e-02, -9.6827e-02,  1.0197e-01,\n",
      "         -6.7054e-02, -7.8792e-02,  3.1787e-02,  5.5464e-03,  5.6397e-02,\n",
      "         -1.0852e-01, -9.5430e-02,  3.9006e-02,  5.0114e-02,  4.7832e-02,\n",
      "          8.9838e-02, -1.0618e-01,  5.3029e-02,  2.7523e-02,  4.6786e-03,\n",
      "          6.1959e-02,  5.9289e-02,  3.1445e-02,  5.6222e-02, -6.9087e-03,\n",
      "          3.4035e-02, -3.2022e-02, -4.2618e-02,  9.1351e-02, -1.7729e-02,\n",
      "         -1.0690e-01, -1.9142e-02,  7.4241e-02,  5.6313e-03, -3.2267e-02,\n",
      "          9.8465e-02,  2.4721e-02, -7.7594e-02, -6.7684e-02, -7.7680e-02,\n",
      "          7.7830e-03, -4.1594e-02,  2.0936e-02, -1.6802e-02,  1.0888e-01,\n",
      "          8.2532e-02,  1.3096e-02, -9.8414e-02,  1.0083e-01,  6.2496e-02,\n",
      "         -8.6154e-02,  1.4039e-02, -1.3320e-02, -1.3772e-02],\n",
      "        [ 3.7825e-02,  7.4108e-02, -3.2385e-03,  4.1628e-02,  9.2338e-02,\n",
      "          1.0764e-01, -3.8435e-02,  3.4705e-02,  9.7241e-02,  7.1435e-02,\n",
      "          6.7991e-02, -2.4629e-02,  3.2375e-02, -9.0957e-02,  3.1113e-02,\n",
      "          6.3348e-02, -7.9766e-02, -8.3789e-02,  6.4728e-02,  6.0701e-02,\n",
      "          4.2332e-02,  9.9243e-04, -5.0841e-03, -7.1244e-02,  7.9455e-02,\n",
      "         -9.6444e-02,  7.4094e-02, -8.8847e-02, -2.7199e-02, -7.6150e-02,\n",
      "         -6.6519e-02,  2.1668e-02,  7.1685e-02, -3.3312e-02,  1.7903e-02,\n",
      "          3.5025e-02,  6.5486e-02,  1.5747e-02, -8.1294e-02,  9.7317e-02,\n",
      "          6.1467e-02, -1.7766e-02,  3.3555e-02,  5.1235e-02,  8.4774e-02,\n",
      "         -9.0748e-02, -5.8098e-02, -7.9453e-02, -3.6601e-02, -3.8326e-02,\n",
      "          7.2067e-02, -8.5543e-02, -9.9752e-02, -7.7318e-02,  5.7824e-02,\n",
      "         -3.3204e-04,  1.1461e-02,  9.2712e-02,  1.7866e-02,  1.0172e-01,\n",
      "         -9.3655e-02, -4.2446e-03, -9.3794e-02,  9.4165e-02, -2.6444e-02,\n",
      "         -1.5927e-02, -7.0965e-02,  1.0324e-01,  6.0285e-02,  6.0018e-02,\n",
      "         -7.3058e-02,  7.7118e-02, -1.3434e-02, -8.7474e-02, -9.6244e-02,\n",
      "         -5.3989e-02, -7.0551e-02,  5.4814e-02,  1.0101e-01, -6.4743e-03,\n",
      "         -7.0909e-02, -4.2616e-02,  1.0251e-01, -8.8882e-02]])), ('fc4.bias', tensor([-0.0220,  0.0127,  0.0137, -0.0198,  0.0721, -0.0544, -0.1024, -0.0100,\n",
      "         0.0478,  0.0572]))])\n"
     ]
    }
   ],
   "source": [
    "print(net.state_dict())\n",
    "print(signet.state_dict())\n",
    "\n",
    "lr = 0.001\n",
    "n_epochs = 1000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer1 = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer2 = optim.SGD(signet.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.988\n",
      "[1,  4000] loss: 0.575\n",
      "[1,  6000] loss: 0.522\n",
      "[1,  8000] loss: 0.449\n",
      "[1, 10000] loss: 0.456\n",
      "[1, 12000] loss: 0.407\n",
      "[1, 14000] loss: 0.406\n",
      "[2,  2000] loss: 0.349\n",
      "[2,  4000] loss: 0.340\n",
      "[2,  6000] loss: 0.374\n",
      "[2,  8000] loss: 0.411\n",
      "[2, 10000] loss: 0.433\n",
      "[2, 12000] loss: 0.430\n",
      "[2, 14000] loss: 0.444\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# for linear model\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer2.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = signet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "# save the model\n",
    "PATH = './cifar_net.pth'\n",
    "torch.save(signet.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load back the model\n",
    "#net = Net()\n",
    "#net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "signet = SigNet()\n",
    "signet.load_state_dict(torch.load(PATH))\n",
    "\n",
    "#outputs = net(images)\n",
    "#_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "#print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              #for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92 %\n"
     ]
    }
   ],
   "source": [
    "total = 0 \n",
    "correct=0\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, labels = data\n",
    "        outputs = signet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of classes     0 : 93 %\n",
      "Accuracy of classes     1 : 97 %\n",
      "Accuracy of classes     2 : 95 %\n",
      "Accuracy of classes     3 : 89 %\n",
      "Accuracy of classes     4 : 91 %\n",
      "Accuracy of classes     5 : 91 %\n",
      "Accuracy of classes     6 : 93 %\n",
      "Accuracy of classes     7 : 97 %\n",
      "Accuracy of classes     8 : 83 %\n",
      "Accuracy of classes     9 : 90 %\n"
     ]
    }
   ],
   "source": [
    "# for signet model\n",
    "class_total = list(0. for i in range(10))\n",
    "class_correct = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in valloader:\n",
    "        images, labels = data\n",
    "        outputs = signet(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        # batch size =4\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of classes %5s : %2d %%' % (\n",
    "        i, 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
